{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DC_Unit12",
      "provenance": [],
      "authorship_tag": "ABX9TyMErg7pn0F56BxuuY/C37P8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhyanshah/MS7333_QTW/blob/master/Case%2012/DC_Unit12_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duOWfvj3UBHB",
        "colab_type": "text"
      },
      "source": [
        "# Case Study 12: Higgs Boson Replica Neural Network\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwnDZ55hU9eV",
        "colab_type": "text"
      },
      "source": [
        "# Methods\n",
        "\n",
        "The first 10.5 MM rows are trained and validated with a 9:1 ratio and the last 500 rows are tested for accuracy, precision, recall and ROC score metrics, not unlike the classification models we ran previously. With the use of tools such as Tensorflow, we can optimize parameters for number of layers, neurons (units), activation function, batch size, kernel initializer, optimizer, learning rate, epsilon, decay rate, and dropout rate. With these optimized parameters, the final model will expand the the number of default epochs from 20 to 500 with early stopping to find the highest ROC score. \n",
        "\n",
        "To give a little more detail on the parameters we are looking to optimize, see below:\n",
        "\n",
        "* No. of Layers - While a single layer can be used to learn problems, utilizing multiple layers allows us to ease the process of our models learning using multilayer perceptrons. \n",
        "* No. of Neurons - like layers, add to the complexity of a model in helping it learn\n",
        "* Activation Function - part of a node that defines the output of that node given a set of inputs. \n",
        "* Batch Size - number of training examples in one forward/backward pass\n",
        "* Kernel Initializer - define the way to set the initial random weights of keras layers\n",
        "* Optimizer - one of two artguments required for compiling a keras module\n",
        "* Learning Rate - a function that takes an epoch index and current learning rate (float) as inputs and returns a new learning rate\n",
        "* Epsilon - the value of the fuzz factor used in numeric expression\n",
        "* Decay Rate - reducing an amount by a consistent peercentage rate over a period of time. \n",
        "* Dropout Rate - excluding random inputs from each update cycle to ensure optimization. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMXWjAOJUfAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#pip install tensorflow"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UueDilLVT92R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "try:\n",
        "    sys.getwindowsversion()\n",
        "except AttributeError:\n",
        "    isWindows = False\n",
        "else:\n",
        "    isWindows = True\n",
        "if isWindows:\n",
        "    import win32api,win32process,win32con\n",
        "    pid = win32api.GetCurrentProcessId()\n",
        "    handle = win32api.OpenProcess(win32con.PROCESS_ALL_ACCESS, True, pid)\n",
        "    win32process.SetPriorityClass(handle, win32process.ABOVE_NORMAL_PRIORITY_CLASS)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "from keras.optimizers import SGD\n",
        "import keras\n",
        "from pprint import pprint\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# in order to have reproducible result\n",
        "from numpy.random import seed\n",
        "import tensorflow\n",
        "tensorflow.random.set_seed(2)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQhFKRnE3wm-",
        "colab_type": "text"
      },
      "source": [
        "We implemented the get_metrics function that will retrive the current model's ROC, accuracy, precision, recall and F1-score metrics for the test dataset. \n",
        "\n",
        "We will build a net functiont hat will set the default parameters for a neural network if undefined in the code. This means we would move forward with a dropout of 0.1, activations is relu, uniform initializer, stochastic gradient descent optimizer, 0.1 learning rate, 1e-6 learning decay, 0.9 momentum, Nesterov method, 5000 batch size, and patience of 2 where accuracy and binary cross entropy loss functions are monitored for the training and validation datasets. \n",
        "\n",
        "Setting the default parameters in the train function will include an early stopping to find accuracy and validation accuracy of the current neural network per epoch for the training and validation datasets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muFwBbnaU09M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_metrics(y_true, y_pred, treshhold=.5):\n",
        "    RocAuc=metrics.roc_auc_score(y_test, y_pred)\n",
        "    y_pred = (y_pred > treshhold).astype(np.int32)\n",
        "    return dict(\n",
        "        RocAuc=\"{:.4f}\".format(metrics.roc_auc_score(y_test, y_pred)),\n",
        "        Accuracy=\"{:.4f}\".format(metrics.accuracy_score(y_test, y_pred)),\n",
        "        Precision=\"{:.4f}\".format(metrics.precision_score(y_test, y_pred)),\n",
        "        Recall=\"{:.4f}\".format(metrics.recall_score(y_test, y_pred)),\n",
        "        F1=\"{:.4f}\".format(metrics.f1_score(y_test, y_pred))\n",
        "    )\n",
        "\n",
        "def net(units,\n",
        "        dropouts=.1,\n",
        "        activations=None,\n",
        "        kernel_initializers='uniform',\n",
        "        optimizer=None\n",
        "    ):\n",
        "\n",
        "    input_dim = x.shape[1]\n",
        "    assert type(units) == list and len(units) and units[-1] == 1\n",
        "    units = units[:]\n",
        "    if activations == None:\n",
        "        activations = ['relu' for _ in units]\n",
        "        activations[-1] = 'sigmoid'\n",
        "    activations = activations[:]\n",
        "    if type(dropouts) != list:\n",
        "        dropouts = [dropouts for _ in range(len(units)-1)]\n",
        "    dropouts = dropouts[:]\n",
        "    if type(kernel_initializers) != list:\n",
        "        kernel_initializers = [kernel_initializers for _ in units]\n",
        "    kernel_initializers = kernel_initializers[:]\n",
        "    assert len(units) == len(kernel_initializers) == len(activations) == len(dropouts)+1\n",
        "    if optimizer is None:\n",
        "        optimizer = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "        \n",
        "    model = Sequential()\n",
        "    \n",
        "    # first layer is special so it is out of the loop\n",
        "    unit = units.pop(0)\n",
        "    activation = activations.pop(0)\n",
        "    try:\n",
        "        dropout = dropouts.pop(0)\n",
        "    except:\n",
        "        dropout = 0.\n",
        "    kernel_initializer = kernel_initializers.pop(0)\n",
        "    \n",
        "    model.add(Dense(unit, input_dim=input_dim, kernel_initializer=kernel_initializer))\n",
        "    if activation:\n",
        "        model.add(Activation(activation))\n",
        "    if dropout:\n",
        "        model.add(Dropout(dropout))\n",
        "    \n",
        "    for unit in units:\n",
        "        activation = activations.pop(0)\n",
        "        try:\n",
        "            dropout = dropouts.pop(0)\n",
        "        except:\n",
        "            dropout = 0.\n",
        "        kernel_initializer = kernel_initializers.pop(0)\n",
        "        \n",
        "        model.add(Dense(unit, kernel_initializer=kernel_initializer))\n",
        "        if activation:\n",
        "            model.add(Activation(activation))\n",
        "        if dropout:\n",
        "            model.add(Dropout(dropout))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=optimizer)\n",
        "    return model\n",
        "\n",
        "default_epochs = 20\n",
        "\n",
        "def train(model, epochs=default_epochs, batch_size=5000, checkpoint=False, patience=2):\n",
        "    history = keras.callbacks.History()\n",
        "    early_stop = keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        min_delta=0,\n",
        "        patience=patience,\n",
        "        verbose=0,\n",
        "        mode='auto')\n",
        "    \n",
        "    callbacks = [history, early_stop]\n",
        "    \n",
        "    if checkpoint:\n",
        "        filepath=\"weights.best.hdf5\"\n",
        "        checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "            filepath='best.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "      #  callbacks.append(checkpoint)\n",
        "    \n",
        "    model.fit(x, y, epochs=epochs, batch_size=batch_size, callbacks=callbacks, validation_split=.1)\n",
        "    result = pd.DataFrame(history.history)\n",
        "    del result['loss']\n",
        "    del result['val_loss']\n",
        "    result = result.rename(columns={\n",
        "        'acc': 'Train Accuracy',\n",
        "        'val_acc': 'Validation Accuracy',\n",
        "        'loss': 'Train Loss',\n",
        "        'val_loss': 'Validation Loss',\n",
        "    })\n",
        "    index = pd.Index(np.arange(epochs), name='Epoch')\n",
        "    result.reindex(index)\n",
        "    result.index += 1\n",
        "    result.index.name = 'Epoch'\n",
        "    result.columns.name = 'Accuracy'\n",
        "    return result"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8KcTn_R66MR",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "\n",
        "To help with the computing power with regards to the size of the dataset, we will convert our default imported float64 values to the float16 to make oiur memory usage more efficient. Our table below shows how our float16 will maintain data integrity while reducing the memory to 608.4 MB. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDMiQlVD7Zr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\n",
        "#!gunzip *.gz\n",
        "\n",
        "data=pd.read_csv(\"HIGGS.csv\",header=None, dtype=np.float64, engine='c')\n",
        "data.columns=['label','lepton pt','lepton eta','lepton phi','missing energy magnitude','missing energy phi',\n",
        "              'jet 1 pt','jet 1 eta','jet 1 phi','jet 1 b-tag','jet 2 pt','jet 2 eta','jet 2 phi','jet 2 b-tag',\n",
        "              'jet 3 pt','jet 3 eta','jet 3 phi','jet 3 b-tag','jet 4 pt','jet 4 eta','jet 4 phi','jet 4 b-tag',\n",
        "              'm_jj','m_jjj','m_lv','m_jlv','m_bb','m_wbb','m_wwbb']\n",
        "\n",
        "data1 = data.max()-data.min()\n",
        "data=data.astype('float16')\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgFmD-vB99qg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install pandas-profiling==2.7.1\n",
        "#conda install -c conda-forge/label/cf202003 pandas-profiling"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU9BzOLH9fbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data2 = data.max()-data.min()\n",
        "\n",
        "#memory = pd.concat([data1, data2], axis=1, join_axes=[data1.index])\n",
        "memory = pd.concat([data1, data2], axis=1)\n",
        "memory.columns=['Before','After']\n",
        "print('Table 1: Data Min and Max from Float64 to Float16')\n",
        "memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ_kHhMv_FDh",
        "colab_type": "text"
      },
      "source": [
        "## Table 1: Float 16 still preserves the data integrity compared to that of float64\n",
        "\n",
        "We plotted all 28 varlables to see how the distrubtions of numerical and categrotical variables to see how much loss we had to the data from converting to a float64 to a float16. Here, we can see that the loss is minimal. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFIJqHYYC39I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.rcParams['figure.max_open_warning']=40\n",
        "colnames=list(data.columns.values)\n",
        "for i in colnames[1:]:\n",
        "    facet = sns.FacetGrid(data, hue='label',aspect=2).set_titles(i)\n",
        "    facet.map(sns.distplot,i)\n",
        "    facet.add_legend()\n",
        "    Title=str([\"Figure \",colnames.index(i),\": \",i,\" Distribution By Label\"])\n",
        "    facet.fig.suptitle(''.join(map(str, list([\"Figure \",colnames.index(i),\": \",i,\" Distribution By Label\"]))))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UshWsoEacC-e",
        "colab_type": "text"
      },
      "source": [
        "In figure 1 through 28, we an see that we have an array of normal and skewed distributions between each of our varaibles at play. \n",
        "\n",
        "Next, let's look at a heatmap of correlations between variables and the Higgs Boson label. The only positive correlation as well as the last 7 derivative kinematic variables which show strong long relations. \n",
        "\n",
        "[Figure 29]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGvSL9PCdgn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax = plt.axes()\n",
        "sns.heatmap(data.corr(), cmap=\"YlGnBu\", ax = ax)\n",
        "ax.set_title('Figure 29: Correlation Heatmap of Higgs Boson Variables')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXfr77pjdqul",
        "colab_type": "text"
      },
      "source": [
        "Figure 29: Most variables are weekly correlated with the Higgs Boson Label for the last 7 variables having any real correlation. \n",
        "\n",
        "We will now split our training and testing datasets into the target and predictor variables at y and x. This will balance our target variable at .05 so that we can choose accuracy as the target metric. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_TL2q3hKWDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# labels must be 0 or 1 and not in between\n",
        "assert not ((data[\"label\"]>0)&(data[\"label\"]<1)).sum()\n",
        "data[\"label\"] = data[\"label\"].astype(np.int16)\n",
        "\n",
        "test_data = data.iloc[-500000:].copy()\n",
        "data = data.iloc[:-500000]\n",
        "\n",
        "y = np.array(data.iloc[:,0])\n",
        "x = np.array(data.iloc[:,1:])\n",
        "\n",
        "x_test = np.array(test_data.iloc[:,1:])\n",
        "y_test = np.array(test_data.iloc[:,0])\n",
        "\n",
        "del test_data, data\n",
        "y.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQOXgGn7D85b",
        "colab_type": "text"
      },
      "source": [
        "The ROC accuracy of our baseline layer and neurode count shows a 0.5 with a high recall, which explains the impact to the F1 score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuugkFxIELDG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = net([1])\n",
        "print(\"Test Dataset :\", get_metrics(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF59yqO-EUhO",
        "colab_type": "text"
      },
      "source": [
        "## 1. Pick 3 or more different architectures (add/subtract layers + neurons) and run the model + score. \n",
        "\n",
        "### Number of Layers Search\n",
        "\n",
        "The default activation is relu to optimize the layers of our neural nethwork by way of the sigmoid havng vanishing gradients that do not work well as relu or elu. We will incomroporate 50 neurons to optimize for layers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyoQ4CTJOuAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams = []\n",
        "for layers in range(1,5):\n",
        "    hparams.append(\n",
        "        dict(units=[50]*layers+[1])\n",
        "    )\n",
        "print('Testing Hyperparameters:')\n",
        "pprint(hparams)\n",
        "\n",
        "trainAcc_layers = {}\n",
        "valAcc_layers = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZBbD0HeE6Ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams = []\n",
        "for layers in range(1,5):\n",
        "    hparams.append(\n",
        "        dict(units=[50]*layers+[1])\n",
        "    )\n",
        "print('Testing Hyperparameters:')\n",
        "pprint(hparams)\n",
        "\n",
        "trainAcc_layers = {}\n",
        "valAcc_layers = {}\n",
        "\n",
        "for hparam in hparams:\n",
        "    title = \"Hyper Params: %s\" % str(hparam)\n",
        "    print(title)\n",
        "    model = net(**hparam)\n",
        "    result = train(model)\n",
        "    trainAcc_layers[title] = result['accuracy']\n",
        "    valAcc_layers[title] = result['val_accuracy']\n",
        "    print(\"Test Dataset :\", get_metrics(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHeVHOk6c2y0",
        "colab_type": "text"
      },
      "source": [
        "### Layers Search Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsZxEJvLc5mh",
        "colab_type": "text"
      },
      "source": [
        "The accuracy increases as more layers are added across the 20 epochs we are using. Using 3 layers of 50 neurons, we saw that there's overfitting int he 4th layer where accuracy stopped as the model failed to continue learning from the training set [fig 30]. Therefore, the best number of layers is 3 with an accuracy of 0.7422 and the validation accuracy of 0.7519."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5c0x_oPdZ2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
        "\n",
        "pd.DataFrame(trainAcc_layers).plot(title='Figure 30A: Layer Training Accuracies', ax=axes[0])\n",
        "pd.DataFrame(valAcc_layers).plot(title='Figure 30B: Layer Validation Accuracies', ax=axes[1])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjnP3Fzpdrp9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_n_layers = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK09FEoTevOy",
        "colab_type": "text"
      },
      "source": [
        "### Number of Neurons "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxnz_xTjexv2",
        "colab_type": "text"
      },
      "source": [
        "We will use the 3 layers we had success with in our previous model, and increase the number of neurons for each layer. We will increase by the same number of neurons to optimize for accuracy and F1 score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOkaUdsffLOF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams = []\n",
        "\n",
        "for units in [100, 200, 400, 800]:\n",
        "    hparams.append(\n",
        "        dict(units=[units]*best_n_layers+[1])\n",
        "    )\n",
        "    \n",
        "print('Testing Hyperparameters:')\n",
        "pprint(hparams)\n",
        "\n",
        "trainAcc_dense = {}\n",
        "valAcc_dense = {}\n",
        "\n",
        "for hparam in hparams:\n",
        "    title = \"Hyperparameters: %s\" % str(hparam)\n",
        "    print(title)\n",
        "    \n",
        "    model = net(**hparam)\n",
        "    result = train(model, epochs=5)\n",
        "    trainAcc_dense[title] = result['accuracy']\n",
        "    valAcc_dense[title] = result['val_accuracy']\n",
        "    print(\"Test Dataset :\", get_metrics(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e_jKbuhj7Z5",
        "colab_type": "text"
      },
      "source": [
        "### Number of neurons search result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaJLpSMfj_LF",
        "colab_type": "text"
      },
      "source": [
        "Accuracy and F1-Score are the highest in the batch of 800 neurons per layer with an accuracy maxing out at 0.7602 [Fig. 31]. The time spent to run each batch increased up to 800, so we will set to 100 neurons moving forward to save time to allow us to optimize the parameters to the final model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TPHIT-Tkpzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
        "\n",
        "pd.DataFrame(trainAcc_dense).plot(title='Figure 31A: Neuron Training Accuracies', ax=axes[0])\n",
        "pd.DataFrame(valAcc_dense).plot(title='Figure 31B: Neuron Validation Accuracies', ax=axes[1])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uem6tN4Ak1Ng",
        "colab_type": "text"
      },
      "source": [
        "### Fig. 31: The most accurate number of neurons is 800 with a training accuracy of 0.760 and a validation accuracy of 0.766."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eOJdtavlE5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#still 800, reduced to 100 to save time and processing\n",
        "best_n_units = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auzcNtGWl8jB",
        "colab_type": "text"
      },
      "source": [
        "## 2. With those 3 architectures, we can run the same architecture with 2 different sigmoid activation functions. \n",
        "\n",
        "### Activations Search\n",
        "\n",
        "The default optimization activation is rely for classification because sigmoid and tanh have vanishing gradients that do not work as well as relu or elu on deeper networks. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8Xjy8I0mamu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams = []\n",
        "for activation in ['elu', 'relu', 'selu']:\n",
        "    hparams.append(\n",
        "        dict(\n",
        "            units=[best_n_units]*best_n_layers+[1],\n",
        "            activations=[activation]*best_n_layers+['sigmoid'],\n",
        "        )\n",
        "    )\n",
        "\n",
        "print('Testing Hyperparameters:')\n",
        "pprint(hparams)\n",
        "\n",
        "trainAcc_activation = {}\n",
        "valAcc_activation = {}\n",
        "\n",
        "for hparam in hparams:\n",
        "    title = \"Hyperparameters: %s\" % str(hparam)\n",
        "    print(title)\n",
        "    model = net(**hparam)\n",
        "    result = train(model, epochs=5)\n",
        "    trainAcc_activation[title] = result['accuracy']\n",
        "    valAcc_activation[title] = result['val_accuracy']\n",
        "    print(\"Test Dataset :\", get_metrics(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxL__CTRq6N3",
        "colab_type": "text"
      },
      "source": [
        "### Activations Search Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBqFEoHGq9pN",
        "colab_type": "text"
      },
      "source": [
        "Relu proved to be the most accurate function with a training set accuracy of 0.74 with a validation test accuracy of 0.75."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvOfaPNyrQTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
        "\n",
        "pd.DataFrame(trainAcc_activation).plot(title='Figure 32A: Activation Training Accuracies', ax=axes[0])\n",
        "pd.DataFrame(valAcc_activation).plot(title='Figure 32B: Activation Validation Accuracies', ax=axes[1])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pz_Mi21vrYVx",
        "colab_type": "text"
      },
      "source": [
        "### Figure 32: Relu is the best activation function by far as opposed to elu and selu. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLrW4JIkrh1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_activation = 'relu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VilASdWtrrrf",
        "colab_type": "text"
      },
      "source": [
        "### 3. Take the best model from 1 and 2 and vary the batch size by at least 2 orders of magnitude. \n",
        "\n",
        "#### Batch Size Search\n",
        "\n",
        "Let's range our batch sizes from 500 to 5000 to see where we are getting our best validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hurqucMusR2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_hparam = dict(\n",
        "    units=[best_n_units]*best_n_layers+[1],\n",
        "    activations=[best_activation]*best_n_layers+['sigmoid'],\n",
        ")\n",
        "\n",
        "trainAcc_batch = {}\n",
        "valAcc_batch = {}\n",
        "\n",
        "for batch_size in 500, 1000, 50000:\n",
        "    title = \"Batch Size: %d\" % batch_size\n",
        "    print(title)\n",
        "    model = net(**best_hparam)\n",
        "    result = train(model, batch_size=batch_size, epochs=10)\n",
        "    trainAcc_batch[title] = result['accuracy']\n",
        "    valAcc_batch[title] = result['val_accuracy']\n",
        "    print(\"Test Dataset :\", get_metrics(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouPU2E38xLzM",
        "colab_type": "text"
      },
      "source": [
        "### Batch Size Search Result\n",
        "\n",
        "The batch size of 1000 had the strongest training set accuracy of 0.75 and a validation accuracy of 0.76."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4TbTX8JxPFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
        "\n",
        "pd.DataFrame(trainAcc_batch).plot(title='Figure 33B: Batch Training Accuracies', ax=axes[0])\n",
        "pd.DataFrame(valAcc_batch).plot(title='Figure 33B: Batch Validation Accuracies', ax=axes[1])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN2FAlR4xrF0",
        "colab_type": "text"
      },
      "source": [
        "#### Figure 33: A batch size of 1000 gives us the highest accuracy set and validation accuacy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJkTLj-Txx_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_batch_size = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z4phpOwx6Sv",
        "colab_type": "text"
      },
      "source": [
        "### Take our best models (based on scores) from parts 1 and 2 and use 3 different kernel initializers with a reasonable batch size (to keep computation costs low).\n",
        "\n",
        "#### Kernel Initializer  Search\n",
        "\n",
        "Initializers for classification models perform better when we have normal distributions in our variables rather than uniform distributions. So we will only focus on initializers with normal distrubtions as well as variance scaling and orthogonal. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qzSTH7GysyY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_hparam = dict(\n",
        "    units=[best_n_units]*best_n_layers+[1],\n",
        "    activations=[best_activation]*best_n_layers+['sigmoid'],\n",
        ")\n",
        "\n",
        "trainAcc_initial = {}\n",
        "valAcc_initial = {}\n",
        "\n",
        "for kernel_initializer in ['VarianceScaling','orthogonal','random_normal','truncated_normal',\n",
        "                           'glorot_normal','lecun_normal','he_normal']:\n",
        "    title = \"Kernel Initializers: %s\" % kernel_initializer\n",
        "    best_hparam['kernel_initializers'] = kernel_initializer\n",
        "    print(title)\n",
        "    \n",
        "    model = net(**best_hparam)\n",
        "    result = train(model, batch_size=best_batch_size, epochs=10)\n",
        "    trainAcc_initial[title] = result['accuracy']\n",
        "    valAcc_initial[title] = result['val_accuracy']\n",
        "    print(\"Test Dataset :\", get_metrics(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Laq8uggLOahg",
        "colab_type": "text"
      },
      "source": [
        "### Kernel Initializer Search Result\n",
        "\n",
        "The random normal initializer is the best with a training accuracy of 0.7619 and a validation accuracy of 0.7613 in Figure 34"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbO4Ij89Omg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
        "\n",
        "pd.DataFrame(trainAcc_initial).plot(title='Figure 34A: Initializer Training Accuracies', ax=axes[0])\n",
        "pd.DataFrame(valAcc_initial).plot(title='Figure 34B: Initializer Validation Accuracies', ax=axes[1])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_evUBHNOq_i",
        "colab_type": "text"
      },
      "source": [
        "#### Figure 34: The Random Normal Initializer is the best with a training accuacy of 0.7619 and a validation accuracy of 0.7613"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dedcLVRO2SP",
        "colab_type": "text"
      },
      "source": [
        "### Take your best 3 results from number 3 and try 3 different optimizers. \n",
        "\n",
        "#### Optimizers Search\n",
        "\n",
        "We will use the default learning rates of the optimizers and see how they are optimized differently. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wXTpA5nPMH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparam = dict(\n",
        "    units=[best_n_units]*best_n_layers+[1],\n",
        "    activations=[best_activation]*best_n_layers+['sigmoid'],\n",
        "    kernel_initializers=best_kernel_initializer\n",
        ")\n",
        "\n",
        "optimizers = dict(\n",
        "    SGD=keras.optimizers.SGD(lr=.01, momentum=0.9, nesterov=True),\n",
        "    Adagrad=keras.optimizers.Adagrad(),\n",
        "    Adamax=keras.optimizers.Adamax(),\n",
        "    Nadam=keras.optimizers.Nadam(),\n",
        "    RMSprop=keras.optimizers.RMSprop(),\n",
        "    Adadelta=keras.optimizers.Adadelta()\n",
        ")\n",
        "\n",
        "trainAcc_optimizer = {}\n",
        "valAcc_optimizer = {}\n",
        "\n",
        "for optimizer_name in optimizers:\n",
        "    title = \"Optimizer: %s\" % optimizer_name\n",
        "    print(optimizer_name)\n",
        "    \n",
        "    model = net(**hparam, optimizer=optimizers[optimizer_name])\n",
        "    result = train(model, batch_size=best_batch_size, epochs=10)\n",
        "    trainAcc_optimizer[title] = result['accuracy']\n",
        "    valAcc_optimizer[title] = result['val_accuracy']\n",
        "    print(\"Test Dataset :\", get_metrics(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtgoIp0NXXGV",
        "colab_type": "text"
      },
      "source": [
        "#### Optimizers Search Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyTyAweMXZeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
        "\n",
        "pd.DataFrame(trainAcc_optimizer).plot(title='Figure 35A: Optimizer Training Accuracies', ax=axes[0])\n",
        "pd.DataFrame(valAcc_optimizer).plot(title='Figure 35B: Optimizer Validation Accuracies', ax=axes[1])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AylAC6MoXdsa",
        "colab_type": "text"
      },
      "source": [
        "### 6. Take all we learned so far and produce a score\n",
        "\n",
        "#### Learning Rate Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiFBICoSXoq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparam = dict(\n",
        "    units=[best_n_units]*best_n_layers+[1],\n",
        "    activations=[best_activation]*best_n_layers+['sigmoid'],\n",
        "    kernel_initializers=best_kernel_initializer\n",
        ")\n",
        "\n",
        "optimizers = dict(\n",
        "    Adamaxlr0015=keras.optimizers.Adamax(lr=0.0015),\n",
        "    Adamaxlr=keras.optimizers.Adamax(),\n",
        "    Adamaxlr0025=keras.optimizers.Adamax(lr=0.0025),\n",
        ")\n",
        "\n",
        "trainAcc_lr = {}\n",
        "valAcc_lr = {}\n",
        "\n",
        "for optimizer_name in optimizers:\n",
        "    title = \"Optimizer: %s\" % optimizer_name\n",
        "    print(optimizer_name)\n",
        "    \n",
        "    model = net(**hparam, optimizer=optimizers[optimizer_name])\n",
        "    result = train(model, batch_size=best_batch_size, epochs=5)\n",
        "    trainAcc_lr[title] = result['accuracy']\n",
        "    valAcc_lr[title] = result['val_accuracy']\n",
        "    print(\"Test Dataset :\", get_metrics(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY-mO9k2XvI3",
        "colab_type": "text"
      },
      "source": [
        "#### Learning Rate Search Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU8GnFz9Xyhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
        "\n",
        "pd.DataFrame(trainAcc_lr).plot(title='Figure 36A: Learning Rate Training Accuracies', ax=axes[0])\n",
        "pd.DataFrame(valAcc_lr).plot(title='Figure 36B: Learning Rate Validation Accuracies', ax=axes[1])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjSFLhY6X15x",
        "colab_type": "text"
      },
      "source": [
        "#### Epsilon Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsrWnSQHX39-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
        "\n",
        "pd.DataFrame(trainAcc_eps).plot(title='Figure 37A: Epsilon Training Accuracies', ax=axes[0])\n",
        "pd.DataFrame(valAcc_eps).plot(title='Figure 37B: Epsilon Validation Accuracies', ax=axes[1])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0YoNCHqX7b3",
        "colab_type": "text"
      },
      "source": [
        "#### Decay Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClmPldUGX_R5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparam = dict(\n",
        "    units=[best_n_units]*best_n_layers+[1],\n",
        "    activations=[best_activation]*best_n_layers+['sigmoid'],\n",
        "    kernel_initializers=best_kernel_initializer\n",
        ")\n",
        "\n",
        "optimizers = dict(\n",
        "    Adamaxde000001=keras.optimizers.Adamax(epsilon=0.000000015,decay=0.000001),\n",
        "    Adamaxde00001=keras.optimizers.Adamax(epsilon=0.000000015,decay=0.00001),\n",
        "    Adamaxde0001=keras.optimizers.Adamax(epsilon=0.000000015,decay=0.0001)\n",
        ")\n",
        "\n",
        "trainAcc_de = {}\n",
        "valAcc_de = {}\n",
        "\n",
        "for optimizer_name in optimizers:\n",
        "    title = \"Optimizer: %s\" % optimizer_name\n",
        "    print(optimizer_name)\n",
        "    \n",
        "    model = net(**hparam, optimizer=optimizers[optimizer_name])\n",
        "    result = train(model, batch_size=best_batch_size, epochs=5)\n",
        "    trainAcc_de[title] = result['accuracy']\n",
        "    valAcc_de[title] = result['val_accuracy']\n",
        "    print(\"Test Dataset :\", get_metrics(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnwMCeAXYClu",
        "colab_type": "text"
      },
      "source": [
        "#### Decay Search Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xecEwG3YFgY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
        "\n",
        "pd.DataFrame(trainAcc_de).plot(title='Figure 38A: Decay Training Accuracies', ax=axes[0])\n",
        "pd.DataFrame(valAcc_de).plot(title='Figure 38B: Decay Validation Accuracies', ax=axes[1])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FpP7T5sYITj",
        "colab_type": "text"
      },
      "source": [
        "#### Dropout Search\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmT_11PcYOAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparam = dict(\n",
        "    units=[best_n_units]*best_n_layers+[1],\n",
        "    activations=[best_activation]*best_n_layers+['sigmoid'],\n",
        "    kernel_initializers=best_kernel_initializer\n",
        ")\n",
        "\n",
        "optimizer = keras.optimizers.Adamax(epsilon=0.000000015)\n",
        "\n",
        "dropouts = dict(\n",
        "    dropout1 = 1,\n",
        "    dropout01 = 0.01,\n",
        "    dropout001 = 0.001,\n",
        "    dropout0001 = 0.0001,\n",
        "    dropout00001 = 0.00001,\n",
        "    dropout000001 = 0.000001,\n",
        "    dropout0000001 = 0.0000001,\n",
        ")\n",
        "\n",
        "trainAcc_do = {}\n",
        "valAcc_do = {}\n",
        "\n",
        "for dropout_name in dropouts:\n",
        "    title = \"Optimizer: %s\" % dropout_name\n",
        "    print(dropout_name)\n",
        "    \n",
        "    model = net(**hparam, optimizer=optimizer, dropouts = dropouts[dropout_name])\n",
        "    result = train(model, batch_size=best_batch_size, epochs=10)\n",
        "    trainAcc_do[title] = result['accuracy']\n",
        "    valAcc_do[title] = result['val_accuracy']\n",
        "    print(\"Test Dataset :\", get_metrics(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmWLcFN7YR2x",
        "colab_type": "text"
      },
      "source": [
        "#### Dropout Search Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzNHCiHvYT3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,8))\n",
        "\n",
        "pd.DataFrame(trainAcc_do).plot(title='Figure 39A: Dropout Training Accuracies', ax=axes[0])\n",
        "pd.DataFrame(valAcc_do).plot(title='Figure 39B: Dropout Validation Accuracies', ax=axes[1])\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dd-b5c0YWzR",
        "colab_type": "text"
      },
      "source": [
        "#### Best Shot Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yij8CjHqYZU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_n_units = 800\n",
        "\n",
        "hparam = dict(\n",
        "    units=[best_n_units]*best_n_layers+[1],\n",
        "    activations=[best_activation]*best_n_layers+['sigmoid'],\n",
        "    kernel_initializers=best_kernel_initializer,\n",
        "    optimizer = keras.optimizers.Adamax(epsilon=0.000000015)\n",
        ")\n",
        "\n",
        "dropout = 0.01\n",
        "\n",
        "model = net(**hparam, dropouts = dropout)\n",
        "result = train(model, batch_size=best_batch_size, epochs=500, checkpoint=True, patience=5)\n",
        "print(\"Test Dataset :\", get_metrics(y_test, model.predict(x_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPKZUY4QYcZy",
        "colab_type": "text"
      },
      "source": [
        "#### Best Shot Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnISEjoWYeYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.DataFrame(result).plot(figsize=(12,8), title='Figure 40: Best Parameter Training and Validation Accuracies')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWaE90RZYiZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "model = keras.models.load_model('best.h5')\n",
        "roc_auc_score(y_test,model.predict(x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8902zwZXKcr",
        "colab_type": "text"
      },
      "source": [
        "References: \n",
        "\n",
        "https://www.heatonresearch.com/2017/06/01/hidden-layers.html#:~:text=Traditionally%2C%20neural%20networks%20only%20had,(not%20the%20next%20layer).\n",
        "\n",
        "https://papers.nips.cc/paper/6372-learning-the-number-of-neurons-in-deep-networks.pdf\n",
        "\n",
        "https://keras.io/api/layers/initializers/\n",
        "\n",
        "https://www.thoughtco.com/exponential-decay-definition-2312215#:~:text=In%20mathematics%2C%20exponential%20decay%20describes,of%20time%20that%20has%20passed."
      ]
    }
  ]
}